{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from numpy.random import RandomState\n",
    "import scipy as scp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomState(92) #To reproduce the same results each time we run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset into a pandas DataFrame\n",
    "df = pd.read_csv('athlete_events.csv', na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Medal'].fillna('No Medal', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all records with missing values\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Data columns and their types\n",
    "df.info()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the second dataset\n",
    "region = pd.read_csv('noc_regions.csv', na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the two datasets\n",
    "merged = pd.merge(df, region, on='NOC', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop('notes', inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if a country exists in dataset\n",
    "\n",
    "if '' in merged.values:\n",
    "    print('\\n This value exists in Dataframe')\n",
    "    \n",
    "else:\n",
    "    print('\\n This value does not exists in Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the median of height\n",
    "merged['Height'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(merged.Height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(merged.Height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.groupby('Sex')['Height'].min()\n",
    "merged.groupby('Sex')['Height'].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.groupby('Sex')['Height'].max()\n",
    "merged.groupby('Sex')['Height'].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Labelling height according to the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Height_class'] = np.where(merged['Height']<175.0, '0', '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These countries in 'l' will be labelled as south asians (SA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['Indonesia', 'Vietnam', 'Philippines', 'Malaysia', 'Sri Lanka', 'Thailand', 'India', 'Pakistan', 'Maldives', 'Afghanistan', 'Bangladesh', 'Bhutan', 'Nepal', 'Brunei', 'Cambodia', 'Laos', 'Myanmar', 'Japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = merged['region'].apply(lambda x: 0 if x in l else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Country']= label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=[merged['Sex']=='M', merged['Country']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1=[merged['Sex']=='M', merged['Country']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Sex'].replace('M', 0, inplace=True)\n",
    "merged['Sex'].replace('F', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "males = merged.loc[merged['Sex'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting the proportion distribution of countries according to height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Males = round(pd.crosstab(males.Country, males.Height_class).div(pd.crosstab(males.Country, males.Height_class).apply(sum,1),0),2)\n",
    "Males.sort_values(by = '1', inplace = True)\n",
    "ax = Males.plot(kind ='bar', color=['#deb887','#8b4513'], title = 'Proportion distribution of countries according to height ', figsize = (8,6))\n",
    "ax.set_xlabel('Country')\n",
    "ax.set_ylabel('Proportion of population')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop(['Team','region','Games','Event','Name'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['NOC', 'City', 'Season','Sport','Medal','Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athletes = pd.get_dummies(merged, columns=categorical_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the height column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(athletes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [i for i in range(m) if i != 5]\n",
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = athletes.iloc[:, idx].values\n",
    "y = athletes.iloc[:, 6].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with neural nets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE SCENARIO\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    model.add(Dense(output_dim = 350, activation = 'relu', input_dim = 371))\n",
    "    # Adding the second hidden layer\n",
    "    model.add(Dense(output_dim = 200, activation = 'relu'))\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=3e-4)\n",
    "    model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size = 10, nb_epoch = 100, validation_split=0.2)\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix for SA countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rows = X_test[:,6]==0\n",
    "X_test_sa = X_test[filter_rows,:]\n",
    "y_test_sa = y_test[filter_rows]\n",
    "y_pred_sa = model.predict(X_test_sa)\n",
    "y_pred_s = np.where(y_pred_sa>=0.5, 1,0)\n",
    "\n",
    "print(classification_report(y_test_sa, y_pred_s))\n",
    "cm_sa = confusion_matrix(y_test_sa, y_pred_s)\n",
    "tn,fp,fn,tp = cm_sa.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy: (tp + tn)/(tp + tn + fp + fn)\n",
    "accuracy = accuracy_score(y_test_sa, y_pred_s)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "#precision: tp/tp+fp\n",
    "precision = precision_score(y_test_sa, y_pred_s)\n",
    "print('Precision: %f' % precision)\n",
    "#recall: tp/tp+fn\n",
    "recall = recall_score(y_test_sa, y_pred_s)\n",
    "print('Recall: %f' % recall)\n",
    "#FNR: 1-recall\n",
    "fnr = 1 - recall\n",
    "print('FNR: %f' % fnr)\n",
    "#FPR: fp/fp+tn\n",
    "fpr = fp / (fp + tn)\n",
    "print('FPR: %f' % fpr)\n",
    "#f1: 2 tp/ (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test_sa, y_pred_s)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC AUC score and Gini coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_sa, y_pred_s)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "Gini_coefficient=2*roc_auc - 1\n",
    "print (\"Gini_coefficient =\",Gini_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix for NSA countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rows = X_test[:,6]==1\n",
    "X_test_nsa = X_test[filter_rows,:]\n",
    "y_test_nsa = y_test[filter_rows]\n",
    "y_pred_nsa = model.predict(X_test_nsa)\n",
    "y_pred_ns = np.where(y_pred_nsa>=0.5, 1,0)\n",
    "\n",
    "print(classification_report(y_test_nsa, y_pred_ns))\n",
    "cm_nsa = confusion_matrix(y_test_nsa, y_pred_ns)\n",
    "tn,fp,fn,tp = cm_nsa.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance measures: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy: (tp + tn)/(tp + tn + fp + fn)\n",
    "accuracy = accuracy_score(y_test_nsa, y_pred_ns)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "#precision: tp/tp+fp\n",
    "precision = precision_score(y_test_nsa, y_pred_ns)\n",
    "print('Precision: %f' % precision)\n",
    "#recall: tp/tp+fn\n",
    "recall = recall_score(y_test_nsa, y_pred_ns)\n",
    "print('Recall: %f' % recall)\n",
    "#FNR: 1-recall\n",
    "fnr = 1 - recall\n",
    "print('FNR: %f' % fnr)\n",
    "#FPR: fp/fp+tn\n",
    "fpr = fp / (fp + tn)\n",
    "print('FPR: %f' % fpr)\n",
    "#f1: 2 tp/ (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test_nsa, y_pred_ns)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC AUC score and Gini coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test_nsa, y_pred_ns)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "Gini_coefficient=2*roc_auc - 1\n",
    "print (\"Gini_coefficient =\",Gini_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
